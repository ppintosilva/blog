---
title: Bayesian priors and dodgy coins
author: Pedro Pinto da Silva
date: '2019-06-05'
slug: bayesian-priors-and-dodgy-coins
categories: []
tags: ['stats-101', 'book-lambert', 'lambert-chapter-5']
draft: false
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#the-problem">The Problem</a></li>
<li><a href="#wanna-see-a-magic-trick">Wanna see a magic trick?</a></li>
<li><a href="#the-posterior-with-a-uniform-prior">The posterior with a uniform prior</a><ul>
<li><a href="#a-single-coin-toss">A single coin toss</a></li>
<li><a href="#multiple-coin-tosses">Multiple coin tosses</a></li>
</ul></li>
<li><a href="#an-informative-prior">An informative prior</a></li>
<li><a href="#the-posterior-predictive-distribution">The Posterior Predictive Distribution</a></li>
</ul>
</div>

<div id="the-problem" class="section level1">
<h1>The Problem</h1>
<p>Today we look at problem 5.1 <em>Dodgy Coins</em>, from Ben Lambert’s book <em>A Student’s Guide to Bayesian Statistics</em>.</p>
<p>3 coins, 1 bag:</p>
<ul>
<li>Coin 1 is biased towards heads (<span class="math inline">\(\theta_1 = 0.75\)</span>)</li>
<li>Coin 2 is fair (<span class="math inline">\(\theta_2 = 0.50\)</span>)</li>
<li>Coin 3 is biased towards tails (<span class="math inline">\(\theta_3 = 0.25\)</span>)</li>
</ul>
</div>
<div id="wanna-see-a-magic-trick" class="section level1">
<h1>Wanna see a magic trick?</h1>
<p>Imagine that we take a coin from the bag and then flip it.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(C = \{1,2,3\}\)</span> denote the identity of the coin (i.e. the outcome of the first <em>experiment</em>)</li>
<li><span class="math inline">\(X = \{T, H\} = \{0,1\}\)</span> denote the outcome of the coin toss</li>
<li><span class="math inline">\(\theta_c\)</span> denote the probability of heads of coin <span class="math inline">\(C = c\)</span>.</li>
</ul>
<p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> are random variables and <span class="math inline">\(\theta_c, \forall c \in C\)</span> is our parameter set (total of 3 parameters).</p>
<p>For a <strong>single</strong> coin toss <span class="math inline">\(X = 1\)</span> (heads), we can calculate the probability of getting heads, given the identity of the coin:</p>
<p><span class="math display">\[
Pr(X = 1 \mid C = c) = \theta_c
\]</span></p>
<p>Using the equivalence relation<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> we can calculate the likelihood of the coin’s identity given the data:</p>
<p><span class="math display">\[
L(C = c \mid X = 1) = Pr(X = 1 \mid C = c) = \theta_c
\]</span></p>
<p>The sum of the likelihood of all parameter values, in this instance, is hence given by:</p>
<p><span class="math display">\[
\sum_{c = 1}^{3} Pr(X=1 \mid C = c) = \theta_1 + \theta_2 + \theta_3 = 1.5
\]</span></p>
<p>And here’s a plot of the likelihood:</p>
<pre class="r"><code>tibble(
  c = 1:3,
  theta = c(0.75, 0.50, 0.25)
) %&gt;%
  ggplot() +
  geom_col(
    aes(c, theta),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     minor_breaks = NULL) +
  theme_bw()</code></pre>
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/likelihood-1.png" width="672" /></p>
<p>Note that the likelihood function is discrete because the hidden quantity is the coin’s identity rather than the probability of heads (we know <span class="math inline">\(\theta_c\)</span> for all <span class="math inline">\(c\)</span>). In the classic coin tossing problem, our knowledge is reversed. We are given the outcome of <span class="math inline">\(n\)</span> coin tosses and want to infer <span class="math inline">\(\theta\)</span> for a single coin. The resulting likelihood function in that case is continuous, as we vary <span class="math inline">\(\theta\)</span> to figure out which values were more likely to generate the observed quantity <span class="math inline">\(x\)</span>. Even though in both problems we’re talking about coins, they end up being quite different!</p>
<p>We can visually see that the maximum likelihood estimate for the coin’s identity in this case is <span class="math inline">\(C_{ML} = \text{argmax}(0.75) = 1\)</span>.</p>
</div>
<div id="the-posterior-with-a-uniform-prior" class="section level1">
<h1>The posterior with a uniform prior</h1>
<p>The goal of Bayesian inference is to compute the posterior, i.e. the probability of the parameters given the data. In this case that means computing:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x)
\]</span></p>
<p>We can do this via <strong>Bayes rule</strong>:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x) = \frac{Pr(X = x \mid C = c) \times Pr(C = c)}{Pr(X = x)}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Pr(X = x \mid C = c)\)</span> is the probability of the data given the parameters, a.k.a the <strong>likelihood</strong></li>
<li><span class="math inline">\(Pr(C = c)\)</span> is the probability of the parameters before seeing the data (prior belief), a.k.a the <strong>prior</strong></li>
<li><span class="math inline">\(Pr(X = x)\)</span> is the probability of the data, a.k.a the marginal probability of the data.</li>
</ul>
<p>By the <a href="https://en.wikipedia.org/wiki/Law_of_total_probability">law of total probability</a>, the denominator can be re-written as:</p>
<p><span class="math display">\[
Pr(X = x) = \sum_{y} Pr(X = x \mid C = y) \times Pr(C = y)
\]</span></p>
<p>where we use <span class="math inline">\(y\)</span> instead of <span class="math inline">\(c\)</span> to denote that the sum is over all possible values of <span class="math inline">\(C\)</span> and avoid confusion between the two variables.</p>
<p>Because the likelihood is not a valid probability distribution, it’s the job of the denominator to make sure that the posterior is one. That is why the denominator is often interpreted as a normalising constant (it can also be interpreted as a probability distribution). We can therefore write that the posterior is proportional to the likelihood times the prior<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x) \propto \Pr(X = x \mid C = c) \times Pr(C = c)
\]</span></p>
<div id="a-single-coin-toss" class="section level2">
<h2>A single coin toss</h2>
<p>Continuing with the example of a single coin toss <span class="math inline">\(x = 1\)</span>, we now assume that the 3 coins are equally likely to be picked from the bag. This is equivalent to choosing the discrete uniform distribution for our prior:</p>
<p><span class="math display">\[
Pr(C = c) = 1/3
\]</span></p>
<p>This is clearly a valid probability distribution as <span class="math inline">\(\sum_{c=1}^{3} Pr(C=c) = 1\)</span>. We usually want to use valid probability distributions for our priors to ensure that the posterior is also one<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>Now that we understand how to calculate the likelihood and prior, we introduce <a href="https://youtu.be/mLxDzAsIg7I">Baye’s Box</a> which help us to compute the denominator (normalising constant), and hence the posterior, by hand:</p>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 1 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 1 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 1)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3/4 = 0.75
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/3 = 0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3/12 = 0.2500
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3/6 = 0.500
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2/4 = 0.50
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
1/3 = 0.333
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2/12 = 0.1667
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2/6 = 0.333
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/4 = 0.25
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/3 = 0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/12 = 0.0833
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/6 = 0.167
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1.5
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
6/12 = 0.5
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p>Looking at the table we can confirm that:</p>
<ul>
<li>The <strong>Likelihood</strong> is not a valid probability distribution (I know I’m repeating this <em>ad nauseum</em> but repetition is at the core of learning)</li>
<li>The <strong>Prior</strong> is a valid (discrete) probability distribution</li>
<li>The <strong>Posterior</strong> is a valid (discrete) probability distribution, i.e. <span class="math inline">\(\sum_{c=1}^{3} Pr(C =c \mid X = 1) = 1\)</span></li>
<li>The denominator is calculated as the sum of the likelihood times the prior over all possible values parameter values, and its job is to normalise the posterior (the normalising constant).</li>
</ul>
</div>
<div id="multiple-coin-tosses" class="section level2">
<h2>Multiple coin tosses</h2>
<p>For <span class="math inline">\(n\)</span> tosses of the same coin, we adapt the random variable <span class="math inline">\(X\)</span> to represent the number of times we record heads in <span class="math inline">\(n\)</span> coin tosses: <span class="math inline">\(X \in \{0,1,\ldots,n\}\)</span>. For a given sample of <span class="math inline">\(x\)</span> heads, we can calculate the likelihood using the probability mass function for the binomial distribution, which models the outcome of <span class="math inline">\(n\)</span> independent <a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trials</a> (only two possible outcomes per trial):</p>
<p><span class="math display" id="eq:binom">\[
\begin{equation}
  L(C = c \mid x) = Pr(X = x \mid C = c) = \binom{n}{x} \theta_{c}^{x}(1-\theta_{c})^{n-x}
  \tag{1}
\end{equation}
\]</span>
Therefore, for two coin tosses and <span class="math inline">\(X = 2\)</span>, we have:</p>
<pre class="r"><code>x     &lt;- 2 # number of heads
c     &lt;- 1:3
theta &lt;-  c(0.75, 0.50, 0.25)

lik   &lt;- dbinom(x = 2, size = 2, p = theta) # likelihood
prior &lt;- rep(1/3, 3)
num   &lt;- lik * prior # numerator
denom &lt;- sum(num) # denominator
post  &lt;- num/denom # posterior</code></pre>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 2)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.5625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.1875
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.6429
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2500
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.0833
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2857
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0208
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0714
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.875
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.292
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p>And here’s a visualisation for an experiment with <span class="math inline">\(n=10\)</span> just because the <code>ggplot + gganimate</code> combo is awesome. Note how the shape of the posterior resembles the one of the likelihood, given a uniform prior:</p>
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/binom%20anim-1.gif" /><!-- --></p>
</div>
</div>
<div id="an-informative-prior" class="section level1">
<h1>An informative prior</h1>
<p>In the previous section we introduced an uninformative prior for <span class="math inline">\(C\)</span>. Despite flat priors being often interpreted as uninformative, its use is discouraged by prominent Bayesian statisticians. Instead, weakly informative priors should often guide an objective analysis (let the data speak for itself). The discussion about the subjectivity and objectivity of Bayesian priors is a long and contentious one. Please refer to the <a href="https://ben-lambert.com/a-students-guide-to-bayesian-statistics/">book</a> and works of <a href="https://scholar.google.co.uk/citations?user=SEOgduoAAAAJ">Gelman</a>, among others, for more details.</p>
<p>Here, the book provides an example of an informative prior, which places much belief on the coin’s identity being biased towards tails:</p>
<p><span class="math display">\[
\begin{align}
Pr(C = 1) &amp;= 1/20\\
Pr(C = 2) &amp;= 5/20\\
Pr(C = 3) &amp;= 14/20
\end{align}
\]</span></p>
<p>We now use Baye’s box again to compute the marginal and posterior distributions, and understand how changing the prior affects the posterior and the resulting inference:</p>
<pre class="r"><code>x     &lt;- 2 # number of heads
c     &lt;- 1:3
theta &lt;- c(0.75, 0.50, 0.25)

lik   &lt;- dbinom(x = 2, size = 2, p = theta) # likelihood
prior &lt;- c(1/20, 5/20, 14/20)
num   &lt;- lik * prior # numerator
denom &lt;- sum(num) # denominator
post  &lt;- num/denom # posterior</code></pre>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 2)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.5625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.05
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0281
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.209
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2500
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.25
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.0625
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.465
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.70
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0437
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.326
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.875
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.134
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p>We can now calculate summary statistics of our posterior distribution:</p>
<pre class="r"><code># Posterior Mean
sum(post * c)</code></pre>
<pre><code>## [1] 2.116279</code></pre>
<pre class="r"><code># Maximum a posterior (MAP) estimate for c
which.max(post)</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code># Maximum likelihood estimate
which.max(lik)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>The posterior mean in this instance is very close to <span class="math inline">\(c=2\)</span>, so if we had to make a decision about the coin’s identity that would be relatively straightforward. In one sense, it is useful because it tells us how close the expected value is to each parameter value. However, we can imagine that, with a different dataset, the resulting mean could lie between parameter values (e.g. 1.5). In that instance, making a decision about the coin’s identity would be a lot more difficult using the mean. The <strong>MAP</strong> (Maximum A Posteriori) or maximum likelihood estimate, on the other hand, always give a straight answer. In this case, they disagree due to the effect of the prior: <span class="math inline">\(C_{MAP}=2\)</span>, <span class="math inline">\(C_{ML}=3\)</span>.</p>
<p>And here’s the same visualisation as before, for an experiment with <span class="math inline">\(n=10\)</span>, for the new prior. Note how the shapes of the posterior and likelihood are now different.</p>
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/binom%20anim%202-1.gif" /><!-- --></p>
</div>
<div id="the-posterior-predictive-distribution" class="section level1">
<h1>The Posterior Predictive Distribution</h1>
<p>Consider the single coin toss example again: <span class="math inline">\(n =1, x = 1\)</span>. If we where to flip the coin a second time, how does first observing heads change our belief about the outcome of the second flip? Are we more, less or equally confident about obtaining heads in the second flip?</p>
<p>We can answer this question via the posterior predictive distribution. With the uniform prior on <span class="math inline">\(C\)</span>, the posterior predictive distribution, in this case, can be obtained using the expression:</p>
<p><span class="math display">\[
Pr(\tilde{X} \mid X = 1) =  \sum_{c=1}^{3} Pr(\tilde{X} \mid C) \times Pr(C \mid X = 1)
\]</span></p>
<p>where <span class="math inline">\(\tilde{X}\)</span> is the result of a new coin flip. But where does this expression come from?</p>
<ul>
<li><span class="math inline">\(Pr(\tilde{X} \mid C)\)</span> is the likelihood of new data <span class="math inline">\(\tilde{X} \in \{0,1\}\)</span> given the coin’s identity <span class="math inline">\(C\)</span>. Guess what? It’s the same expression that we used before: the binomial distribution!</li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp; Pr(\tilde{X} = 1 \mid C = 1) = \theta_1 \\
&amp; Pr(\tilde{X} = 1 \mid C = 2) = \theta_2 \\
&amp; \ldots \\
&amp; Pr(\tilde{X} = 0 \mid C = 3) = 1 - \theta_3
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(Pr(C \mid X = 1)\)</span> is the posterior that we’ve also calculated above:</li>
</ul>
<p><span class="math display">\[
\begin{align}
&amp; Pr(C = 1 \mid X = 1) = 3/6\\
&amp; Pr(C = 2 \mid X = 1) = 2/6\\
&amp; Pr(C = 3 \mid X = 1) = 1/6
\end{align}
\]</span></p>
<p>You can interpret the posterior predictive distribution as the average of the probability of a single coin toss (likelihood) weighted by the posterior. For instance, if we want to find the probability of a new coin toss resulting heads, then we average <span class="math inline">\(\theta_c\)</span> over all values of <span class="math inline">\(c\)</span>, weighted by how likely we think that each parameter is to occur given the previously observed outcome. It is the posterior that provides the weights for each parameter!</p>
<p>In general, to find the posterior predictive distribution, you sample a parameter value from the posterior, feed that into your likelihood function, repeat the process many times and :boom: you have a probability distribution capable of predicting new data based on the old data!</p>
<pre class="r"><code>x     &lt;- 1 # number of heads
c     &lt;- 1:3
theta &lt;- c(0.75, 0.50, 0.25)
post  &lt;- c(3/6, 2/6, 1/6)

new_x &lt;- c(0,1)
lik_new_x &lt;- matrix(c(1-theta, theta),
                    nrow = 2,
                    byrow = TRUE)

post.pred &lt;- lik_new_x %*% matrix(post, nrow = 3)

rownames(post.pred) &lt;- c(&quot;Pr(new_X = 0 | X = 1) = &quot;, &quot;Pr(new_X = 1 | X = 1) = &quot;)
colnames(post.pred) &lt;- &quot;&quot;

post.pred</code></pre>
<pre><code>##                                   
## Pr(new_X = 0 | X = 1) =  0.4166667
## Pr(new_X = 1 | X = 1) =  0.5833333</code></pre>
<p>So, if we had to guess the next coin flip, based on the first outcome being heads (and our model: prior + likelihood), we’d probably put our money on heads.</p>
<p>Model evaluation is about checking how well it can reproduce the data generating process. Maybe our likelihood function is not adequate. Maybe our choice of prior is poor. The posterior predictive distribution is one way to address these questions. But there is a lot more to model evaluation than just computing the posterior predictive distribution (in most instances we can only approximate it by sampling from the posterior). In short, how good your model is depends on its ability to generate extreme values present in the data. Read more about this topic on chapter 10 of the book.</p>
<p>In the meantime, happy modelling ∩ coding!</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>see chapter 4 of the book for more details<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>see chapter 6 and 7 of <a href="https://ben-lambert.com/a-students-guide-to-bayesian-statistics/">Ben Lambert’s book</a> for more details.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>please refer to chapters 5 the book for a more in depth discussion on the validity of priors<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
