---
title: The Bayesian denominator of too many coin flips
author: Pedro Pinto da Silva
date: '2019-06-19'
slug: the-bayesian-denominator-of-too-many-coin-flips
categories: ['stats']
tags: ['stats-101', 'book-lambert', 'lambert-chapter-6']
draft: false
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
library(tidyverse)
library(gganimate)
library(kableExtra)
library(formattable)

theme_color = "#5D8AA8"
```

# The problem

Today we look at problem 6.1 _Too many coin flips_, from Ben Lambert's book _A Student's Guide to Bayesian Statistics_.

This time around we have 2 coins, whose nature we are unsure about (probability of heads). However, we know that each coin $i$ can be fair ($\theta_i = 0.5$) or biased towards heads ($\theta_i = 0.9$).

# A uniform start

Suppose that:

- we flip each coin twice: $n = 2$
- we place a discrete uniform prior on the coin's identity $i$

**Q:** What is the joint distribution of the data and the coins' identity?

First things first, let $X_i$ be the number of heads in $n$ throws of coin $i$. Then the joint distribution of interest is:

$$
Pr(X_1, X_2, \theta_1, \theta_2) = Pr(X_{1:2}, \theta_{1:2})
$$

If we assume that the two coin throws are independent (i.e. the outcome of throwing coin 1 does not affect the outcome throwing coin 2), then:

$$
Pr(X_{1:2}, \theta_{1:2}) = \prod_{i = 1}^{2} Pr(X_i, \theta_i) = \prod_{i = 1}^{2} Pr(X_i \mid \theta_i) \times Pr(\theta_i)
$$


Note that, for each coin, we can write the joint $Pr(X_i, \theta_i)$ as the product of the conditional distribution of the data given the parameters $Pr(X_i \mid \theta_i)$ and the distribution of the coin's identity $Pr(\theta_i)$. Therefore, if we know how to calculate these two terms, then we can easily compute the whole thing:

- given the coins identity, the binomial distribution can be used to model the outcome of $n$ independent coin flips (we've seen this in a [previous post](/2019/06/05/bayesian-priors-and-dodgy-coins/)):

$$
Pr(x_i \mid \theta_i) = \binom{n}{x_i} \theta_{i}^{x_i}(1-\theta_{i})^{n-x_i}
$$


- the uniform assumption over the coin's identity tells us how to calculate $Pr(\theta_i)$:

$$
\begin{align}
Pr(\theta_i = 1) = 1/2 , \quad Pr(\theta_i = 2) = 1/2\\
\end{align}
$$

```{r joint, echo = FALSE, eval = FALSE}
coins_tibble <- function(n, theta) {
  ncoins <- length(theta)
  
  comb <- expand.grid(rep(list(theta = theta, x = 0:ncoins), ncoins))
  column_names <- paste0(c("theta_", "x_"), sapply(1:ncoins, function(i) rep(i, ncoins)))
  colnames(comb) <- column_names
  
  comb <-
    comb %>%
    as_tibble() %>%
    rowid_to_column(var = "case")
  
  comb_theta <- 
    comb %>%
    gather(paste0("theta_", 1:ncoins), key = "theta_i", value = "theta") %>%
    separate(theta_i, c("dummy","i"), "_") %>%
    mutate(i = as.numeric(i)) %>%
    select(-c(paste0("x_", 1:ncoins), dummy))
  
  comb_x <- 
    comb %>%
    gather(paste0("x_", 1:ncoins), key = "x_i", value = "x") %>%
    separate(x_i, c("dummy","i"), "_") %>%
    mutate(i = as.numeric(i)) %>%
    select(-c(paste0("theta_", 1:ncoins), dummy))
  
  list(
    comb = comb,
    df   = 
      inner_join(comb_theta, comb_x, by = c("case", "i")) %>%
      mutate(n = n)
  )
}

prior <- c(1/2, 1/2)

l = coins_tibble(2, c(0.5, 0.9))

l_joint <-
  l$df %>%
  mutate(prior = prior[i]) %>%
  mutate(p = dbinom(x, n, theta) * prior) %>%
  group_by(case) %>%
  summarise(joint_p = prod(p)) %>%
  inner_join(l$comb, by = "case")

l_joint

 l_joint %>%
  summarise(sum(joint_p))
```

The joint defines a probability distribution over all possible combinations of $X_1$, $X_2$, $\theta_1$ and $\theta_2$. Because the random variables are discrete, we can enumrate all the possible combinations and write a bit of code to do the computation for us. We can expect a total of $3 \times 3 \times 2 \times 2 = 36$ different probabilities:

```{r joint dist}
joint <- 
  tibble(
    x1     = c(0, 1, 2),
    x2     = c(0, 1, 2),
    theta1 = c(0.5, 0.9, 0.5),
    theta2 = c(0.5, 0.9, 0.5)
  ) %>%
  expand(x1, x2, theta1, theta2) %>%
  mutate(
    n      = 2,
    prior1 = 1/2,
    prior2 = 1/2
  ) %>%
  mutate(
    lik1 = dbinom(x1, n, theta1),
    lik2 = dbinom(x2, n, theta2),
    num1 = lik1 * prior1,
    num2 = lik2 * prior2,
  ) %>%
  mutate(p = num1 * num2)

```

```{r joint table, echo = FALSE}
coln = joint %>% unite("x1x2", x1, x2, sep = ",") %>% pull(x1x2) %>% unique()
rown = joint %>% unite("t1t2", theta1, theta2, sep = ",") %>% pull(t1t2) %>% unique()

tabela <- 
  joint %>%
  pull(p) %>%
  matrix(
    nrow = 4,
    ncol = 9,
    dimnames = c(
      list(rown, coln)
    )
  )

tabela %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  column_spec(1, bold = T, background = theme_color, color = "white") %>%
  add_header_above(c("$\\theta_1,\\theta_2$", "X1,X2" = 9), escape = FALSE)
```

---

We can verify that the above distribution is a valid probability distribution:


```{r verify valid dist}
joint %>%
  summarise(sum(p))
```

# One and only likelihood

We now assume that we've observed $X_1 = 2$ and $X_2 = 1$. How likely is this data, given our _probability model_ of coin flips (binomial)?

We can answer this question via the _likelihood_:

$$
\begin{align}
L(\theta_{1:2} \mid x_{1:2}) &= \prod_{i=1}^{2} L(\theta_i \mid x_i)\\
&= \prod_{i=1}^{2} Pr(X_i = x_i \mid \theta_i)\\
&= Pr(X_1 = 2 \mid \theta_1) \times Pr(X_2 = 1 \mid \theta_2)
\end{align}
$$

We bring out the _likelihood_ when we're presented with a single dataset. We fix the data $X_1 = x_1$, $X_2 = x_2$ and vary the parameters $\theta_1$ and $\theta_2$. **Note** that we've used the same expression, times the prior $Pr(\theta_i)$, to compute the joint; but that we did that for all possible datasets, i.e. combinations of $X_1$ and $X_2$!


```{r likelihood}
likelihood <- 
  joint %>%
  filter(x1 == 2 & x2 == 1) %>%
  select(-c(n, prior1, prior2, num1, num2, p)) %>%
  mutate(lik = lik1 * lik2)

likelihood %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(which.max(likelihood$lik), bold = T, color = "white", background = "red") %>%
  column_spec(1:length(likelihood), width_min = "50pt")
```

</br></br></br>

---

Note that the likelihood is not a valid probability distribution:


```{r likelihood not valid}
likelihood %>%
  summarise(sum(lik))
```


Now, if we were to select point estimates for $\theta_1,\theta_2$ (single values), we would probably choose the ones that maximise the likelihood. The _maximum likelihood estimator_ for the parameters can simply be determined by looking at the table above: $\theta_1 = 0.9, \theta_2 = 0.5$.

The result makes sense intuitively: the model prefers coin 1 to be biased towards heads because it flipped heads twice and coin 2 to be fair as it flipped once heads and once tails. If we were approaching this problem from a frequentist point of view, our job would be done here. However, from a Bayesian perspective, we're just getting started.

# The devil is in the denominator

In Bayesian Statistics, it is often said that the _devil is in the denominator_. What is the denominator, or marginal likelihood, exactly?

We can obtain the probability of the data $Pr_(x_1, x_2)$ by summing over all possible values of the parameters:

$$
\begin{align}
Pr(x_{1:2}) &= \sum_{\theta_1} \sum_{\theta_2} Pr(x_{1:2}, \theta_{1:2})\\
&= \sum_{\theta_1}\sum_{\theta_2}Pr(x_{1:2} \mid \theta_{1:2}) Pr(\theta_{1:2})\\
&= \sum_{\theta_1}Pr(x_1 \mid \theta_1)Pr(\theta_1) \sum_{\theta_2} Pr(x_2 \mid \theta_2)Pr(\theta_2)
\end{align}
$$

This is equivalent to selecting the columns of the joint $X_1 = 2$ and $X_2 = 1$ and summing the probabilities for all values of $\theta_1$ and $\theta_2$.

```{r numerator}
numerator <- 
  joint %>%
  filter(x1 == 2 & x2 == 1)
```

```{r numerator table, echo = FALSE}

numerator %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  column_spec(1:length(numerator), width_min = "40pt")
```

```{r denominator}
denominator <- 
  numerator %>%
  summarise(value = sum(p)) %>%
  pull(value)

denominator
```

As, we'll see above, the job of the denominator is to ensure that the posterior distribution is a valid probability distribution.

# The posterior

The posterior aims to compute the update belief about the coin's identities, after we've observed the 2 coins flips for each coin. This is now trivial, since we've worked out all of its components: the _likelihood_, the _prior_ and the _denominator_:

$$
Pr(\theta_{1:2} \mid x_{1:2}) = \frac{L(\theta_{1:2} \mid x_{1:2}) \times Pr(\theta_{1:2})}{Pr(x_{1:2})}
$$

```{r posterior}
posterior <- 
  joint %>%
  filter(x1 == 2 & x2 == 1) %>%
  mutate(denom = denominator) %>%
  mutate(post = p/denom)

posterior %>%
  select(-c(n, prior1, prior2, num1, num2, lik1, lik2, p, denom)) %>%
  mutate(post = format(post, digits = 3)) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  column_spec(1:(length(posterior)-9), width_min = "40pt")
```
<br/><br/><br/>

---


We can verify that the posterior is a valid probability distribution:

```{r posterior valid}
posterior %>%
  summarise(sum(post))
```

As before, we now may want to compute point estimates for our parameters. In this case, we're interested in the mean of $\theta_1$. In the univariate case, this is simply a weighted average over the posterior: $E(\theta \mid x) = \sum_{\theta} \theta . Pr(\theta \mid x)$ In this case, we need to first sum the posterior over $\theta_2$:

$$
E(\theta_1 \mid x_{1:2}, \theta_2) = \sum_{\theta_1} \theta_1 \sum_{\theta_2} Pr(\theta_{1}, \theta_2 \mid x_{1:2})
$$

```{r posterior mean}
posterior_mean <- 
  posterior %>%
  group_by(theta1) %>%
  summarise(partial = sum(post)) %>%
  summarise(sum(theta1 * partial))

posterior_mean
```

And despite the result not falling into one of the two possible values for the coins identity, the mean is definetely pointing towards the coin being biased in the favour of heads.
