---
title: Bayesian priors and dodgy coins
author: Pedro Pinto da Silva
date: '2019-06-05'
slug: bayesian-priors-and-dodgy-coins
categories: []
tags: ['stats-101', 'book-lambert', 'lambert-chapter-5']
draft: false
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#the-problem">The Problem</a></li>
<li><a href="#wanna-see-a-magic-trick">Wanna see a magic trick?</a></li>
<li><a href="#the-posterior-with-a-uniform-prior">The posterior with a uniform prior</a><ul>
<li><a href="#a-single-coin-toss">A single coin toss</a></li>
<li><a href="#multiple-coin-tosses">Multiple coin tosses</a></li>
</ul></li>
<li><a href="#an-informative-prior">An informative prior</a></li>
<li><a href="#the-posterior-predictive-distribution">The Posterior Predictive Distribution</a></li>
</ul>
</div>

<div id="the-problem" class="section level1">
<h1>The Problem</h1>
<p>Today we look at problem 5.1 <em>Dodgy Coins</em>, from Ben Lambert’s book <em>A Student’s Guide to Bayesian Statistics</em>.</p>
<p>3 coins, 1 bag:</p>
<ul>
<li>Coin 1 is biased towards heads (<span class="math inline">\(\theta_1 = 0.75\)</span>)</li>
<li>Coin 2 is fair (<span class="math inline">\(\theta_2 = 0.50\)</span>)</li>
<li>Coin 3 is biased towards tails (<span class="math inline">\(\theta_3 = 0.25\)</span>)</li>
</ul>
</div>
<div id="wanna-see-a-magic-trick" class="section level1">
<h1>Wanna see a magic trick?</h1>
<p>Imagine that we take a coin from the bag and then flip it.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(C = \{1,2,3\}\)</span> denote the identity of the coin (i.e. the outcome of the first <em>experiment</em>)</li>
<li><span class="math inline">\(X = \{T, H\} = \{0,1\}\)</span> denote the outcome of the coin toss</li>
<li><span class="math inline">\(\theta_c\)</span> denote the probability of heads of coin <span class="math inline">\(C = c\)</span>.</li>
</ul>
<p>Note that <span class="math inline">\(X\)</span> and <span class="math inline">\(C\)</span> are random variables and <span class="math inline">\(\theta_c\)</span> is our parameter set <span class="math inline">\(\forall c \in C\)</span>.</p>
<p>We can use the binomial distribution to model the outcome of <span class="math inline">\(n\)</span> (independent) coin tosses. If there was only a coin in the bag, this leads to the standard binomial likelihood function that you might be familiar with (don’t worry if you’re not!):</p>
<p><span class="math display">\[
L(\theta \mid x) = Pr(X = x \mid \theta) = \binom{n}{x} \theta^{x}(1-\theta)^{n-x}
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the probability of heads of the single hypothetical coin. That means that the likelihood, using the equivalence relation, is function of <span class="math inline">\(\theta\)</span> given the data. And the data is composed of just two values: <span class="math inline">\(n\)</span>, the number of tosses, and <span class="math inline">\(x\)</span>, the number of observed heads.
From now on we simply use the notation <span class="math inline">\(Pr(X = x)\)</span> to denote the probability mass of <span class="math inline">\(X\)</span> given <span class="math inline">\(\theta\)</span>. Making this explicit is useful at first, especially for begginers, but then it just adds clutter to our notation, so we do without it.</p>
<p>If we consider a <strong>single</strong> coin toss with <span class="math inline">\(x = 1\)</span> (heads), the above likelihood simplifies to:</p>
<p><span class="math display">\[
L(\theta \mid x) = Pr(X = 1) = \theta
\]</span></p>
<p>However, in this instance, we need to take into account the identity of the coin, which we don’t know with certainty. That means, that we need to include the outcome of the first <em>experiment</em> (<span class="math inline">\(C = c\)</span>) in our model.</p>
<p>For <span class="math inline">\(n\)</span> coin tosses of the <strong>same coin</strong> <span class="math inline">\(c\)</span>, the likelihood function is:</p>
<p><span class="math display" id="eq:binom">\[
\begin{equation}
  L(\theta_c \mid x) = Pr(X = x \mid C = c) = \binom{n}{x} \theta_{c}^{x}(1-\theta_{c})^{n-x}
  \tag{1}
\end{equation}
\]</span></p>
<p>For a single coin toss <span class="math inline">\(x = 1\)</span> (heads), this simplifies to:</p>
<p><span class="math display">\[
L(\theta_c \mid x) = Pr(X = 1 \mid C = c) = \theta_c
\]</span></p>
<p>The sum of the likelihood of all parameter values, in this instance, is hence given by:</p>
<p><span class="math display">\[
\sum_{c = 1}^{3} Pr(X=1 \mid C = c) = \theta_1 + \theta_2 + \theta_3 = 1.5
\]</span></p>
<p>And here’s a plot of the likelihood:</p>
<pre class="r"><code>tibble(
  c = 1:3,
  theta = c(0.25, 0.50, 0.75)
) %&gt;%
  ggplot() +
  geom_col(
    aes(c, theta),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     minor_breaks = NULL) +
  theme_bw()</code></pre>
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/likelihood-1.png" width="672" /></p>
<p>Note that, the likelihood function is discrete because we know the probabilities of heads (values of <span class="math inline">\(\theta_c\)</span>). In the classic coin tossing problem, we are given the outcome of <span class="math inline">\(n\)</span> coin tosses and want to infer <span class="math inline">\(\theta\)</span> for a single coin. The resulting likelihood function in that case is continuous, as we vary <span class="math inline">\(\theta\)</span>. In this instance, we want to infer the hidden identity of the coin <span class="math inline">\(C\)</span>, and <span class="math inline">\(\theta\)</span> can only take one of 3 values. These two problems end up being quite different!</p>
<p>We can visually see that the maximum likelihood estimate for the coin’s identity in this case is <span class="math inline">\(\theta_{ML} = \theta_c = 0.75\)</span>.</p>
</div>
<div id="the-posterior-with-a-uniform-prior" class="section level1">
<h1>The posterior with a uniform prior</h1>
<p>The goal of Bayesian inference is to compute the posterior, i.e. the probability of our parameters (and model) given the data. In this case that means computing:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x)
\]</span></p>
<p>We can do this via <strong>Bayes rule</strong>:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x) = \frac{Pr(X = x \mid C = c) \times Pr(C = c)}{Pr(X = x)}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Pr(X = x \mid C = c)\)</span> is the probability of the data given the parameters, a.k.a the <strong>likelihood</strong></li>
<li><span class="math inline">\(Pr(C = c)\)</span> is the probability of the parameters before seeing the data (prior belief), a.k.a the <strong>prior</strong></li>
<li><span class="math inline">\(Pr(X = x)\)</span> is the probability of the data, a.k.a the marginal probability of the data.</li>
</ul>
<p>By the law of total probability, the denominator can be re-written as:</p>
<p><span class="math display">\[
Pr(X = x) = \sum_{c} Pr(X = x \mid C = c) \times Pr(C = c)
\]</span></p>
<p>Because the likelihood is not a valid probability distribution, it’s the job of the denominator to make sure that the posterior is a valid probability distribution. That is why the denominator is often interpreted as a normalising constant (it can also be interpreted as a probability distribution). We can therefore write that the posterior is proportional to the likelihood times the prior<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<p><span class="math display">\[
Pr(C = c \mid X = x) \propto \Pr(X = x \mid C = c) \times Pr(C = c)
\]</span></p>
<div id="a-single-coin-toss" class="section level2">
<h2>A single coin toss</h2>
<p>Continuing with the example of a single coin toss <span class="math inline">\(x = 1\)</span>, we now assume that the 3 coins are equally likely to be picked. This is equivalent to picking the discrete uniform distribution for our prior. Now that we understand how to calculate the likelihood and prior, we introduce <a href="https://youtu.be/mLxDzAsIg7I">Baye’s Box</a> which help us to compute the denominator (normalising constant), and hence the posterior, by hand:</p>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 1 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 1 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 1)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
<span class="math inline">\(\theta_1\)</span> = 1/4
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/12
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
(1/12)/(6/12) = 1/6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
<span class="math inline">\(\theta_2\)</span> = 2/4
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
1/3
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2/12
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
(2/12)/(6/12) = 2/6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
<span class="math inline">\(\theta_2\)</span> = 3/4
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1/3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3/12
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
(3/12)/(6/12) = 3/6
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
6/4
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
6/12
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p>Looking at the table we can confirm that:</p>
<ul>
<li>The <strong>Likelihood</strong> is not a valid probability distribution (I know I’m repeating this <em>ad nauseum</em> but repetition is at the core of learning)</li>
<li>The <strong>Posterior</strong> is a valid (discrete) probability distribution, i.e. <span class="math inline">\(\sum_{c=1}^{3} Pr(C =c \mid X = 1) = 1\)</span></li>
<li>The denominator is a constant, with value <span class="math inline">\(1/6\)</span>.</li>
</ul>
</div>
<div id="multiple-coin-tosses" class="section level2">
<h2>Multiple coin tosses</h2>
<p>For <span class="math inline">\(n\)</span> tosses of the same coin, our random variable <span class="math inline">\(X\)</span> now represents the number of times we record heads: <span class="math inline">\(X \in \{0,1,\ldots,n\}\)</span>. And for a given sample of <span class="math inline">\(x\)</span> heads, we can calculate the likelihood using the formula for the binomial probability mass function, introduced above <a href="#eq:binom">(1)</a>.</p>
<p>Therefore, for two coin tosses and <span class="math inline">\(X = 2\)</span>, we have:</p>
<pre class="r"><code>x     &lt;- 2 # number of heads
c     &lt;- 1:3
theta &lt;-  c(0.25, 0.50, 0.75)

lik   &lt;- dbinom(x = 2, size = 2, p = theta) # likelihood
prior &lt;- rep(1/3, 3)
num   &lt;- lik * prior # numerator
denom &lt;- sum(num) # denominator
post  &lt;- num/denom # posterior</code></pre>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 2)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0208
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0714
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2500
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.0833
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2857
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.5625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.333
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.1875
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.6429
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.875
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.292
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p>And here’s a visualisation just because the <code>ggplot + gganimate</code> combo is awesome:</p>
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/binom%20anim-1.gif" /><!-- --></p>
</div>
</div>
<div id="an-informative-prior" class="section level1">
<h1>An informative prior</h1>
<p>In the previous section we introduced an uninformative prior for <span class="math inline">\(C\)</span>. It is intuitive why a uniform prior is uninformative in this case as, in some sense, it lets the data speak for itself, and the resulting shape of the posterior is fully determined by the likelihood. However, a flat prior is not always uninformati</p>
<pre class="r"><code>x     &lt;- 2 # number of heads
c     &lt;- 1:3
theta &lt;- c(0.25, 0.50, 0.75)

lik   &lt;- dbinom(x = 2, size = 2, p = theta) # likelihood
prior &lt;- c(1/20, 5/20, 14/20)
num   &lt;- lik * prior # numerator
denom &lt;- sum(num) # denominator
post  &lt;- num/denom # posterior</code></pre>
<table class="table table-hover" style="float: left; margin-right: 10px;">
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Parameter
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Likelihood <span class="math inline">\(\times\)</span> Prior
</th>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: #5D8AA8 !important;text-align: center;font-size: 15px;">
Posterior
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
C
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C =c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(X = 2 \mid C = c) \times Pr(C=c)\)</span>
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;font-size: 13px;">
<span class="math inline">\(Pr(C = c \mid X = 2)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
1
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.05
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.00313
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.0068
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
2
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.2500
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.25
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.06250
</td>
<td style="text-align:left;background-color: #fafafa !important;font-size: 14px;">
0.1361
</td>
</tr>
<tr>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
3
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.5625
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.70
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.39375
</td>
<td style="text-align:left;background-color: transparent !important;font-size: 14px;">
0.8571
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
Total
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.875
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
0.459
</td>
<td style="text-align:left;font-weight: bold;background-color: #f1f1f1 !important;font-size: 15px;">
1
</td>
</tr>
</tbody>
</table>
<hr />
<p><img src="/post/2019-06-04-bayesian-priors-and-dodgy-coins_files/figure-html/binom%20anim%202-1.gif" /><!-- --></p>
<p>We can now calculate summary statistics of our posterior distribution:</p>
<pre class="r"><code># Posterior Mean
mean(post)</code></pre>
<pre><code>## [1] 0.3333333</code></pre>
<pre class="r"><code># Maximum a posterior (MAP) estimate = Maximum Likelihood estimate
max(post)</code></pre>
<pre><code>## [1] 0.8571429</code></pre>
<p>The posterior mean in this instance is not a very useful summary statistic or point estimate, as its value lies between the probability mass assigned to parameters values <span class="math inline">\(c=2\)</span> and <span class="math inline">\(c=3\)</span>. This is because of the discrete nature of <span class="math inline">\(C\)</span>. If we had to make a decision about which of the coin is more likely to have been picked, this would be very difficult to do using the mean. The <strong>MAP</strong> (Maximum A Posteriori) or maximum likelihood estimate, on the other hand, would give us a straight answer (<span class="math inline">\(c=3\)</span>).</p>
</div>
<div id="the-posterior-predictive-distribution" class="section level1">
<h1>The Posterior Predictive Distribution</h1>
<p>One way to evaluate the model is to check how well it can reproduce the data generating process. We can do this via the posterior predictive distribution. Considering the single coin toss example again: <span class="math inline">\(n =1, x = 1\)</span>, with the uniform prior on C, the posterior predictive distribution can be obtained using the expression:</p>
<p><span class="math display">\[
Pr(\tilde{X} \mid X = 1) =  \sum_{c=1}^{3} Pr(\tilde{X} \mid C) \times Pr(C \mid X = 1)
\]</span></p>
<p>Where does this expression come from?</p>
<pre class="r"><code>x     &lt;- 1 # number of heads
c     &lt;- 1:3
theta &lt;- c(0.25, 0.50, 0.75)
post  &lt;- c(1/6, 2/6, 3/6)

xtilde &lt;- c(0,1)
lik_xtilde &lt;- matrix(c(1-theta, theta), nrow = 2, byrow = TRUE)
post.pred &lt;- lik_xtilde %*% matrix(post, nrow = 3)
rownames(post.pred) &lt;- c(&quot;Pr(new_X | X = 0) = &quot;, &quot;Pr(new_X | X = 1) = &quot;)
colnames(post.pred) &lt;- &quot;&quot;

post.pred</code></pre>
<pre><code>##                               
## Pr(new_X | X = 0) =  0.4166667
## Pr(new_X | X = 1) =  0.5833333</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Please refer to <a href="https://ben-lambert.com/a-students-guide-to-bayesian-statistics/">Ben Lambert’s book</a> for more details.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
