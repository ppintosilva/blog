---
title: Bayesian priors and dodgy coins
author: Pedro Pinto da Silva
date: '2019-06-04'
slug: bayesian-priors-and-dodgy-coins
categories: []
tags: ['stats-101', 'book-lambert', 'lambert-chapter-5']
draft: true
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
library(tidyverse)
library(kableExtra)
library(formattable)

theme_color = "#5D8AA8"
```

# The Problem

Today we look at problem 5.1 _Dodgy Coins_, from Ben Lambert's book _A Student's Guide to Bayesian Statistics_.

3 coins, 1 bag:

- Coin 1 is biased towards heads ($\theta_1 = 0.75$)
- Coin 2 is fair ($\theta_2 = 0.50$)
- Coin 3 is biased towards tails ($\theta_3 = 0.25$)

# Wanna see a magic trick?

Imagine that we take a coin from the bag and then flip it.

Let:

- $C = \{1,2,3\}$ denote the identity of the coin (i.e. the outcome of the first _experiment_)
- $X = \{T, H\} = \{0,1\}$ denote the outcome of the coin toss
- $\theta_c$ denote the probability of heads of coin $C = c$.

Note that $X$ and $C$ are random variables and $\theta_c$ is our parameter set $\forall c \in C$.

We can use the binomial distribution to model the outcome of $n$ (independent) coin tosses. If there was only a coin in the bag, this leads to the standard binomial likelihood function that you might be familiar with (don't worry if you're not!):

$$
L(\theta \mid x) = Pr(X = x \mid \theta) = \binom{n}{x} \theta^{x}(1-\theta)^{n-x}
$$

where $\theta$ is the probability of heads of the single hypothetical coin. That means that the likelihood, using the equivalence relation, is function of $\theta$ given the data. And the data is composed of just two values: $n$, the number of tosses, and $x$, the number of observed heads.
From now on we simply use the notation $Pr(X = x)$ to denote the probability mass of $X$ given $\theta$.  Making this explicit is useful at first, especially for begginers, but then it just adds clutter to our notation, so we do without it.

If we consider a **single** coin toss with $x = 1$ (heads), the above likelihood simplifies to:

$$
L(\theta \mid x) = Pr(X = 1) = \theta
$$

However, in this instance, we need to take into account the identity of the coin, which we don't know with certainty. That means, that we need to include the outcome of the first _experiment_ ($C = c$) in our model.

For $n$ coin tosses of the **same coin** $c$, the likelihood function is:

$$
L(\theta_c \mid x) = Pr(X = x \mid C = c) = \binom{n}{x} \theta_{c}^{x}(1-\theta_{c})^{n-x}
$$

For a single coin toss $x = 1$ (heads), this simplifies to:

$$
L(\theta_c \mid x) = Pr(X = 1 \mid C = c) = \theta_c
$$

The sum of the likelihood of all parameter values, in this instance, is hence given by:

$$
\sum_{c = 1}^{3} Pr(X=1 \mid C = c) = \theta_1 + \theta_2 + \theta_3 = 1.5
$$

And here's a plot of the likelihood:

```{r likelihood}
tibble(
  c = 1:3,
  theta = c(0.25, 0.50, 0.75)
) %>%
  ggplot() +
  geom_col(
    aes(c, theta),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     minor_breaks = NULL) +
  theme_bw()
  
```

Note that, the likelihood function is discrete because we know the probabilities of heads (values of $\theta_c$). In the classic coin tossing problem, we are given the outcome of $n$ coin tosses and want to infer $\theta$ for a single coin. The resulting likelihood function in that case is continuous, as we vary $\theta$. In this instance, we want to infer the hidden identity of the coin $C$, and $\theta$ can only take one of 3 values. These two problems end up being quite different!

We can visually see that the maximum likelihood estimate for the coin's identity in this case is $\theta_{ML} = \theta_c = 0.75$.

# The posterior with a uniform prior

The goal of Bayesian inference is to compute the posterior, i.e. the probability of our parameters (and model) given the data. In this case that means computing:

$$
Pr(C = c \mid X = x)
$$

We can do this via **Bayes rule**:

$$
Pr(C = c \mid X = x) = \frac{Pr(X = x \mid C = c) \times Pr(C = c)}{Pr(X = x)}
$$

where:

- $Pr(X = x \mid C = c)$ is the probability of the data given the parameters, a.k.a the **likelihood**
- $Pr(C = c)$ is the probability of the parameters before seeing the data (prior belief), a.k.a the **prior**
- $Pr(X = x)$ is the probability of the data, a.k.a the marginal probability of the data.
  
By the law of total probability, the denominator can be re-written as:

$$
Pr(X = x) = \sum_{c} Pr(X = x \mid C = c) \times Pr(C = c)
$$

Because the likelihood is not a valid probability distribution, it's the job of the denominator to make sure that the posterior is a valid probability distribution. That is why the denominator is often interpreted as a normalising constant (it can also be interpreted as a probability distribution). We can therefore write that the posterior is proportional to the likelihood times the prior^[Please refer to [Ben Lambert's book](https://ben-lambert.com/a-students-guide-to-bayesian-statistics/) for more details.]:

$$
Pr(C = c \mid X = x) \propto \Pr(X = x \mid C = c) \times Pr(C = c)
$$

## A single coin toss

Continuing with the example of a single coin toss $x = 1$, we now assume that the 3 coins are equally likely to be picked. This is equivalent to picking the discrete uniform distribution for our prior. Now that we understand how to calculate the likelihood and prior, we introduce [Baye's Box](https://youtu.be/mLxDzAsIg7I) which help us to compute the denominator (normalising constant), and hence the posterior, by hand:

```{r bayes box, echo = FALSE}

tibble(
  Parameter = c("C", "1" , "2", "3", "Total"),
  Likelihood = c("$Pr(X = 1 \\mid C =c)$", "$\\theta_1$ = 1/4", "$\\theta_2$ = 2/4", "$\\theta_2$ = 3/4", "6/4"),
  Prior = c("$Pr(C = c)$", "1/3", "1/3", "1/3", "1"),
  `Likelihood $\\times$ Prior` = c("$Pr(X = 1 \\mid C = c) \\times Pr(C=c)$", "1/12", "2/12", "3/12", "6/12"),
  Posterior = c("$Pr(C = c \\mid X = 1)$", "(1/12)/(6/12) = 1/6", "(2/12)/(6/12) = 2/6", "(3/12)/(6/12) = 3/6", "1")
) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(5, bold = T, background = "#f1f1f1", font_size = 15) %>%
  row_spec(1, font_size = 13)

```

---

Looking at the table we can confirm that:

- The **Likelihood** is not a valid probability distribution (I know I'm repeating this _ad nauseum_ but repetition is at the core of learning)
- The **Posterior** is a valid (discrete) probability distribution, i.e. $\sum_{c=1}^{3} Pr(C =c \mid X = 1) = 1$
- The denominator is a constant, with value $1/6$.

## Multiple coin tosses

For $n$ tosses of the same coin, our random variable $X$ now represents the number of times we record heads as the outcome of each toss: $X \in \{0,1,\ldots,n\}$.

We recover the likelihood function from above

For two coin tosses and $X = 2$, we have: # REF #

```{r two coin tosses}
x     = 2 # number of heads
c     = 1:3
theta = c(0.25, 0.50, 0.75)

lik   = dbinom(x = 2, size = 2, p = theta) # likelihood
prior = rep(1/3, 3)
num   = lik * prior # numerator
denom = sum(num) # denominator
post  = num/denom # posterior
```

```{r bayes box 2, echo = FALSE}
tibble(
  Parameter = c("C", "1" , "2", "3", "Total"),
  Likelihood = c("$Pr(X = 1 \\mid C =c)$", format(lik, digits = 3), as.character(sum(lik))),
  Prior = c("$Pr(C = c)$", format(prior, digits = 3), as.character(sum(prior))),
  `Likelihood $\\times$ Prior` = c("$Pr(X = 1 \\mid C = c) \\times Pr(C=c)$", format(num, digits = 3), format(denom, digits = 3)),
  Posterior = c("$Pr(C = c \\mid X = 1)$", format(post, digits = 3), as.character(sum(post)))
) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(5, bold = T, background = "#f1f1f1", font_size = 15) %>%
  row_spec(1, font_size = 13)

```

And we wrap up this section with a nice animation with the awesome `ggplot + gganimate` combo:



# An informative prior

