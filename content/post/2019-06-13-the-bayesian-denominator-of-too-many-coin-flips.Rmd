---
title: The Bayesian denominator of too many coin flips
author: Pedro Pinto da Silva
date: '2019-06-19'
slug: the-bayesian-denominator-of-too-many-coin-flips
categories: ['stats']
tags: ['stats-101', 'book-lambert', 'lambert-chapter-6']
draft: true
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
library(tidyverse)
library(gganimate)
library(kableExtra)
library(formattable)

theme_color = "#5D8AA8"
```

# The problem

Today we look at problem 6.1 _Too many coin flips_, from Ben Lambert's book _A Student's Guide to Bayesian Statistics_.

We continue working with coins.

This time around we have 2 coins, whose nature we are unsure about (probability of heads). However, we know that each coin $i$ can be fair ($\theta_i = 0.5$) or biased towards heads ($\theta_i = 0.9$).

# A uniform start

Suppose that:

- we flip each coin twice: $n = 2$
- we place a discrete uniform prior on the coin's identity $i$

**Q:** What is the joint distribution of the data and the coins' identity?

First things first, let $X_i$ be the number of heads in $n$ throws of coin $i$. Then the joint distribution of interest is:

$$
Pr(X_1, X_2, \theta_1, \theta_2) = Pr(X_{1:2}, \theta_{1:2})
$$

If we assume that the two coin throws are independent (i.e. the outcome of throwing coin 1 does not affect the outcome throwing coin 2), then:

$$
Pr(X_{1:2}, \theta_{1:2}) = \prod_{i = 1}^{2} Pr(X_i, \theta_i) = \prod_{i = 1}^{2} Pr(X_i \mid \theta_i) \times Pr(\theta_i)
$$


Note that, for each coin, we can write the joint ($Pr(X_i, \theta_i)$) as the product of the conditional distribution of the data given the parameters $Pr(X_i \mid \theta_i)$ and the distribution of the coin's identity ($Pr(\theta_i)$). Therefore, if we know how to calculate these two terms, then we can easily compute the whole thing:

- given the coins identity, the binomial distribution can be used to model the outcome of $n$ independent coin flips (we've seen in a [previous post](/2019/06/05/bayesian-priors-and-dodgy-coins/)):

$$
Pr(x_i \mid \theta_i) = \binom{n}{x_i} \theta_{i}^{x_i}(1-\theta_{i})^{n-x_i}
$$


- the uniform assumption over the coin's identity tells us how to calculate $Pr(\theta_i)$:

$$
\begin{align}
Pr(\theta_i = 1) = 1/2 , \quad Pr(\theta_i = 2) = 1/2\\
\end{align}
$$


```{r joint, echo = FALSE, eval = FALSE}
coins_tibble <- function(n, theta) {
  ncoins <- length(theta)
  
  comb <- expand.grid(rep(list(theta = theta, x = 0:ncoins), ncoins))
  column_names <- paste0(c("theta_", "x_"), sapply(1:ncoins, function(i) rep(i, ncoins)))
  colnames(comb) <- column_names
  
  comb <-
    comb %>%
    as_tibble() %>%
    rowid_to_column(var = "case")
  
  comb_theta <- 
    comb %>%
    gather(paste0("theta_", 1:ncoins), key = "theta_i", value = "theta") %>%
    separate(theta_i, c("dummy","i"), "_") %>%
    mutate(i = as.numeric(i)) %>%
    select(-c(paste0("x_", 1:ncoins), dummy))
  
  comb_x <- 
    comb %>%
    gather(paste0("x_", 1:ncoins), key = "x_i", value = "x") %>%
    separate(x_i, c("dummy","i"), "_") %>%
    mutate(i = as.numeric(i)) %>%
    select(-c(paste0("theta_", 1:ncoins), dummy))
  
  list(
    comb = comb,
    df   = 
      inner_join(comb_theta, comb_x, by = c("case", "i")) %>%
      mutate(n = n)
  )
}

prior <- c(1/2, 1/2)

l = coins_tibble(2, c(0.5, 0.9))

l_joint <-
  l$df %>%
  mutate(prior = prior[i]) %>%
  mutate(p = dbinom(x, n, theta) * prior) %>%
  group_by(case) %>%
  summarise(joint_p = prod(p)) %>%
  inner_join(l$comb, by = "case")

l_joint

 l_joint %>%
  summarise(sum(joint_p))
```

And a little code to do the computation for us:

```{r joint dist}
joint <- 
  tibble(
    x1     = c(0, 1, 2),
    x2     = c(0, 1, 2),
    theta1 = c(0.5, 0.9, 0.5),
    theta2 = c(0.5, 0.9, 0.5)
  ) %>%
  expand(x1, x2, theta1, theta2) %>%
  mutate(
    n     = 2,
    prior1 = 1/2,
    prior2 = 1/2
  ) %>%
  mutate(
    lik1 = dbinom(x1, n, theta1),
    lik2 = dbinom(x2, n, theta2),
    num1 = lik1 * prior1,
    num2 = lik2 * prior2,
  ) %>%
  mutate(p = num1 * num2)

```

```{r joint table, echo = FALSE}
joint %>%
  select(-c(n, prior1, prior2, lik1, lik2, num1, num2)) %>%
  knitr::kable(escape = FALSE)
  # print(n = 36)
  # pull(p) %>%
  # matrix(
  #   nrow = 3^2,
  #   ncol = 2^2,
  #   dimnames = list(
  #     joint %>% unite("x1_x2", )
  #   )) %>%
  # knitr::kable(escape = FALSE)
```

We can verify that the above distribution is a valid probability distribution:

```{r verify valid dist}
joint %>%
  summarise(sum(p))
```

# One likelihood to rule them all

$$
\begin{align}
L(\theta_{1:2} \mid x_{1:2}) &= \prod_{i=1}^{2} L(\theta_i \mid x_i)\\
&= \prod_{i=1}^{2} Pr(X_i = x_i \mid \theta_i)\\
&= Pr(X_1 = 2 \mid \theta_1) \times Pr(X_2 = 1 \mid \theta_2)
\end{align}
$$

```{r likelihood}
likelihood <- 
  joint %>%
  filter(x1 == 2 & x2 == 1) %>%
  select(-c(n, prior1, prior2, num1, num2, p)) %>%
  mutate(lik = lik1 * lik2)

likelihood %>%
  knitr::kable(escape = FALSE) %>%
  row_spec(which.max(likelihood$lik), bold = T, color = "white", background = "red")
```

The MLE for the parameters $\theta_1,\theta_2$ is the value of these parameters that maximises the likelihood: $\theta_1 = 0.9, \theta_2 = 0.5$.

# The devil is in the denominator

$$
\begin{align}
Pr(x_{1:2}) &= \sum_{\theta_1} \sum_{\theta_2} Pr(x_{1:2}, \theta_{1:2})\\
&= \sum_{\theta_1}\sum_{\theta_2}Pr(x_{1:2} \mid \theta_{1:2}) Pr(\theta_{1:2})\\
&= \sum_{\theta_1}Pr(x_1 \mid \theta_1)Pr(\theta_1) \sum_{\theta_2} Pr(x_2 \mid \theta_2)Pr(\theta_2)
\end{align}
$$

```{r denominator}
numerator <- 
  joint %>%
  filter(x1 == 2 & x2 == 1)

numerator %>%
  knitr::kable(escape = FALSE)

denominator <- 
  numerator %>%
  summarise(value = sum(p)) %>%
  pull(value)

denominator
```

# Hey, Mr Postman!

$$
Pr(\theta_{1:2} \mid x_{1:2}) = \frac{L(\theta_{1:2} \mid x_{1:2}) \times Pr(\theta_{1:2})}{Pr(x_{1:2})}
$$

```{r posterior}
posterior <- 
  joint %>%
  filter(x1 == 2 & x2 == 1) %>%
  mutate(denom = denominator) %>%
  mutate(post = p/denom)

posterior %>%
  select(-c(n, prior1, prior2, num1, num2, p, lik1, lik2, p, denom)) %>%
  knitr::kable(escape = FALSE)

posterior %>%
  summarise(sum(post))
```

Mean of $\theta_1$:

$$
E(\theta_1 \mid x_{1:2}, \theta_2) = \sum_{\theta_1} \theta_1 \sum_{\theta_2} Pr(\theta_{1}, \theta_2 \mid x_{1:2})
$$

```{r posterior mean}
posterior_mean <- 
  posterior %>%
  group_by(theta1) %>%
  summarise(partial = sum(post)) %>%
  summarise(sum(theta1 * partial))

posterior_mean
```

# The new coin in the block
