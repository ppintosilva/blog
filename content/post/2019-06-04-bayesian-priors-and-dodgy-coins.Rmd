---
title: Bayesian priors and dodgy coins
author: Pedro Pinto da Silva
date: '2019-06-05'
slug: bayesian-priors-and-dodgy-coins
categories: []
tags: ['stats-101', 'book-lambert', 'lambert-chapter-5']
draft: false
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache =TRUE)
library(tidyverse)
library(gganimate)
library(kableExtra)
library(formattable)

theme_color = "#5D8AA8"
```

# The Problem

Today we look at problem 5.1 _Dodgy Coins_, from Ben Lambert's book _A Student's Guide to Bayesian Statistics_.

3 coins, 1 bag:

- Coin 1 is biased towards heads ($\theta_1 = 0.75$)
- Coin 2 is fair ($\theta_2 = 0.50$)
- Coin 3 is biased towards tails ($\theta_3 = 0.25$)

# Wanna see a magic trick?

Imagine that we take a coin from the bag and then flip it.

Let:

- $C = \{1,2,3\}$ denote the identity of the coin (i.e. the outcome of the first _experiment_)
- $X = \{T, H\} = \{0,1\}$ denote the outcome of the coin toss
- $\theta_c$ denote the probability of heads of coin $C = c$.

Note that $X$ and $C$ are random variables and $\theta_c$ is our parameter set $\forall c \in C$.

We can use the binomial distribution to model the outcome of $n$ (independent) coin tosses. If there was only a coin in the bag, this leads to the standard binomial likelihood function that you might be familiar with (don't worry if you're not!):

$$
L(\theta \mid x) = Pr(X = x \mid \theta) = \binom{n}{x} \theta^{x}(1-\theta)^{n-x}
$$

where $\theta$ is the probability of heads of the single hypothetical coin. That means that the likelihood, using the equivalence relation, is function of $\theta$ given the data. And the data is composed of just two values: $n$, the number of tosses, and $x$, the number of observed heads.
From now on we simply use the notation $Pr(X = x)$ to denote the probability mass of $X$ given $\theta$.  Making this explicit is useful at first, especially for begginers, but then it just adds clutter to our notation, so we do without it.

If we consider a **single** coin toss with $x = 1$ (heads), the above likelihood simplifies to:

$$
L(\theta \mid x) = Pr(X = 1) = \theta
$$

However, in this instance, we need to take into account the identity of the coin, which we don't know with certainty. That means, that we need to include the outcome of the first _experiment_ ($C = c$) in our model.

For $n$ coin tosses of the **same coin** $c$, the likelihood function is:

$$
\begin{equation}
  L(\theta_c \mid x) = Pr(X = x \mid C = c) = \binom{n}{x} \theta_{c}^{x}(1-\theta_{c})^{n-x}
  (\#eq:binom)
\end{equation}
$$

For a single coin toss $x = 1$ (heads), this simplifies to:

$$
L(\theta_c \mid x) = Pr(X = 1 \mid C = c) = \theta_c
$$

The sum of the likelihood of all parameter values, in this instance, is hence given by:

$$
\sum_{c = 1}^{3} Pr(X=1 \mid C = c) = \theta_1 + \theta_2 + \theta_3 = 1.5
$$

And here's a plot of the likelihood:

```{r likelihood}
tibble(
  c = 1:3,
  theta = c(0.25, 0.50, 0.75)
) %>%
  ggplot() +
  geom_col(
    aes(c, theta),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     minor_breaks = NULL) +
  theme_bw()
  
```

Note that, the likelihood function is discrete because we know the probabilities of heads (values of $\theta_c$). In the classic coin tossing problem, we are given the outcome of $n$ coin tosses and want to infer $\theta$ for a single coin. The resulting likelihood function in that case is continuous, as we vary $\theta$. In this instance, we want to infer the hidden identity of the coin $C$, and $\theta$ can only take one of 3 values. These two problems end up being quite different!

We can visually see that the maximum likelihood estimate for the coin's identity in this case is $\theta_{ML} = \theta_c = 0.75$.

# The posterior with a uniform prior

The goal of Bayesian inference is to compute the posterior, i.e. the probability of our parameters (and model) given the data. In this case that means computing:

$$
Pr(C = c \mid X = x)
$$

We can do this via **Bayes rule**:

$$
Pr(C = c \mid X = x) = \frac{Pr(X = x \mid C = c) \times Pr(C = c)}{Pr(X = x)}
$$

where:

- $Pr(X = x \mid C = c)$ is the probability of the data given the parameters, a.k.a the **likelihood**
- $Pr(C = c)$ is the probability of the parameters before seeing the data (prior belief), a.k.a the **prior**
- $Pr(X = x)$ is the probability of the data, a.k.a the marginal probability of the data.
  
By the law of total probability, the denominator can be re-written as:

$$
Pr(X = x) = \sum_{c} Pr(X = x \mid C = c) \times Pr(C = c)
$$

Because the likelihood is not a valid probability distribution, it's the job of the denominator to make sure that the posterior is a valid probability distribution. That is why the denominator is often interpreted as a normalising constant (it can also be interpreted as a probability distribution). We can therefore write that the posterior is proportional to the likelihood times the prior^[Please refer to [Ben Lambert's book](https://ben-lambert.com/a-students-guide-to-bayesian-statistics/) for more details.]:

$$
Pr(C = c \mid X = x) \propto \Pr(X = x \mid C = c) \times Pr(C = c)
$$

## A single coin toss

Continuing with the example of a single coin toss $x = 1$, we now assume that the 3 coins are equally likely to be picked. This is equivalent to picking the discrete uniform distribution for our prior. Now that we understand how to calculate the likelihood and prior, we introduce [Baye's Box](https://youtu.be/mLxDzAsIg7I) which help us to compute the denominator (normalising constant), and hence the posterior, by hand:

```{r bayes box, echo = FALSE}

tibble(
  Parameter = c("C", "1" , "2", "3", "Total"),
  Likelihood = c("$Pr(X = 1 \\mid C =c)$", "$\\theta_1$ = 1/4", "$\\theta_2$ = 2/4", "$\\theta_2$ = 3/4", "6/4"),
  Prior = c("$Pr(C = c)$", "1/3", "1/3", "1/3", "1"),
  `Likelihood $\\times$ Prior` = c("$Pr(X = 1 \\mid C = c) \\times Pr(C=c)$", "1/12", "2/12", "3/12", "6/12"),
  Posterior = c("$Pr(C = c \\mid X = 1)$", "(1/12)/(6/12) = 1/6", "(2/12)/(6/12) = 2/6", "(3/12)/(6/12) = 3/6", "1")
) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(5, bold = T, background = "#f1f1f1", font_size = 15) %>%
  row_spec(1, font_size = 13)

```

---

Looking at the table we can confirm that:

- The **Likelihood** is not a valid probability distribution (I know I'm repeating this _ad nauseum_ but repetition is at the core of learning)
- The **Posterior** is a valid (discrete) probability distribution, i.e. $\sum_{c=1}^{3} Pr(C =c \mid X = 1) = 1$
- The denominator is a constant, with value $1/6$.

## Multiple coin tosses

For $n$ tosses of the same coin, our random variable $X$ now represents the number of times we record heads: $X \in \{0,1,\ldots,n\}$. And for a given sample of $x$ heads, we can calculate the likelihood using the formula for the binomial probability mass function, introduced above \@ref(eq:binom).

Therefore, for two coin tosses and $X = 2$, we have:

```{r two coin tosses}
x     <- 2 # number of heads
c     <- 1:3
theta <-  c(0.25, 0.50, 0.75)

lik   <- dbinom(x = 2, size = 2, p = theta) # likelihood
prior <- rep(1/3, 3)
num   <- lik * prior # numerator
denom <- sum(num) # denominator
post  <- num/denom # posterior
```

```{r bayes box 2, echo = FALSE}
tibble(
  Parameter = c("C", "1" , "2", "3", "Total"),
  Likelihood = c("$Pr(X = 2 \\mid C =c)$", format(lik, digits = 3), as.character(sum(lik))),
  Prior = c("$Pr(C = c)$", format(prior, digits = 3), as.character(sum(prior))),
  `Likelihood $\\times$ Prior` = c("$Pr(X = 2 \\mid C = c) \\times Pr(C=c)$", format(num, digits = 3), format(denom, digits = 3)),
  Posterior = c("$Pr(C = c \\mid X = 2)$", format(post, digits = 3), as.character(sum(post)))
) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(5, bold = T, background = "#f1f1f1", font_size = 15) %>%
  row_spec(1, font_size = 13)

```
---

And here's a visualisation just because the `ggplot + gganimate` combo is awesome:

```{r binom anim, echo = FALSE}

n <- 10

simul <- 
  lapply(0:n, function(x)
    tibble(
      heads = x,
      c     = c,
      theta = theta,
      likelihood = dbinom(x = x, size = n, p = theta),
      prior = rep(1/length(c), length(c)),
    )
  ) %>%
  bind_rows() %>%
  group_by(heads) %>%
  mutate(
    numerator = likelihood * prior,
    denominator = sum(numerator),
    posterior = numerator/denominator
  ) %>%
  gather(key = "key", value = "value",
         likelihood, prior, numerator, denominator, posterior) %>%
  mutate(key = factor(key,
                     levels = c("likelihood", "prior", "posterior",
                                "numerator", "denominator")))

anim_p <- 
  simul %>%
  ggplot() +
  geom_col(
    aes(x = c, y = value),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  # scale_y_continuous(breaks = seq(0, 1, 0.25),
  #                    minor_breaks = NULL) +
  theme_bw() +
  facet_wrap(~key) +
  # gganimate
  labs(title = "Number of heads X = {frame_time} in n = 10 tosses") +
  transition_time(heads) +
  ease_aes('linear')
  
animate(anim_p)
```

# An informative prior

In the previous section we introduced an uninformative prior for $C$. It is intuitive why a uniform prior is uninformative in this case as, in some sense, it lets the data speak for itself, and the resulting shape of the posterior is fully determined by the likelihood. However, a flat prior is not always uninformati

```{r two coin tosses informative prior}
x     <- 2 # number of heads
c     <- 1:3
theta <- c(0.25, 0.50, 0.75)

lik   <- dbinom(x = 2, size = 2, p = theta) # likelihood
prior <- c(1/20, 5/20, 14/20)
num   <- lik * prior # numerator
denom <- sum(num) # denominator
post  <- num/denom # posterior
```

```{r bayes box 3, echo = FALSE}
tibble(
  Parameter = c("C", "1" , "2", "3", "Total"),
  Likelihood = c("$Pr(X = 2 \\mid C =c)$", format(lik, digits = 3), as.character(sum(lik))),
  Prior = c("$Pr(C = c)$", format(prior, digits = 3), as.character(sum(prior))),
  `Likelihood $\\times$ Prior` = c("$Pr(X = 2 \\mid C = c) \\times Pr(C=c)$", format(num, digits = 3), format(denom, digits = 3)),
  Posterior = c("$Pr(C = c \\mid X = 2)$", format(post, digits = 3), as.character(sum(post)))
) %>%
  knitr::kable(escape = FALSE) %>%
  kable_styling(bootstrap_options = c("stripped", "hover"), full_width = T, position = "float_left") %>%
  row_spec(0, bold = T, background = theme_color, color = "white", font_size = 15, align = "c") %>%
  row_spec(c(1,3), bold = F, background = "#fafafa", font_size = 14) %>%
  row_spec(c(2,4), bold = F, background = "transparent", font_size = 14) %>%
  row_spec(5, bold = T, background = "#f1f1f1", font_size = 15) %>%
  row_spec(1, font_size = 13)

```
---

```{r binom anim 2, echo = FALSE}

n <- 10

simul <- 
  lapply(0:n, function(x)
    tibble(
      heads = x,
      c     = c,
      theta = theta,
      prior = prior,
      likelihood = dbinom(x = x, size = n, p = theta),
    )
  ) %>%
  bind_rows() %>%
  group_by(heads) %>%
  mutate(
    numerator = likelihood * prior,
    posterior = numerator/sum(numerator)
  ) %>%
  select(-numerator) %>%
  gather(key = "key", value = "value",
         likelihood, prior, posterior) %>%
  mutate(key = factor(key,
                     levels = c("likelihood", "prior", "posterior")))

anim_p <- 
  simul %>%
  ggplot() +
  geom_col(
    aes(x = c, y = value),
    fill = theme_color
  ) +
  scale_x_continuous(breaks = 1:3,
                     minor_breaks = NULL) +
  # scale_y_continuous(breaks = seq(0, 1, 0.25),
  #                    minor_breaks = NULL) +
  theme_bw() +
  facet_wrap(~key) +
  # gganimate
  labs(title = "Number of heads X = {frame_time} in n = 10 tosses") +
  transition_time(heads) +
  ease_aes('linear')
  
animate(anim_p)
```

We can now calculate summary statistics of our posterior distribution:

```{r summary posterior}
# Posterior Mean
mean(post)

# Maximum a posterior (MAP) estimate = Maximum Likelihood estimate
max(post)
```

The posterior mean in this instance is not a very useful summary statistic or point estimate, as its value lies between the probability mass assigned to parameters values $c=2$ and $c=3$.  This is because of the discrete nature of $C$. If we had to make a decision about which of the coin is more likely to have been picked, this would be very difficult to do using the mean. The **MAP** (Maximum A Posteriori) or maximum likelihood estimate, on the other hand, would give us a straight answer ($c=3$).

# The Posterior Predictive Distribution

One way to evaluate the model is to check how well it can reproduce the data generating process. We can do this via the posterior predictive distribution. Considering the single coin toss example again: $n =1, x = 1$, with the uniform prior on C, the posterior predictive distribution can be obtained using the expression:

$$
Pr(\tilde{X} \mid X = 1) =  \sum_{c=1}^{3} Pr(\tilde{X} \mid C) \times Pr(C \mid X = 1)
$$

Where does this expression come from?

```{r posterior pred}
x     <- 1 # number of heads
c     <- 1:3
theta <- c(0.25, 0.50, 0.75)
post  <- c(1/6, 2/6, 3/6)

xtilde <- c(0,1)
lik_xtilde <- matrix(c(1-theta, theta), nrow = 2, byrow = TRUE)
post.pred <- lik_xtilde %*% matrix(post, nrow = 3)
rownames(post.pred) <- c("Pr(new_X | X = 0) = ", "Pr(new_X | X = 1) = ")
colnames(post.pred) <- ""

post.pred
```